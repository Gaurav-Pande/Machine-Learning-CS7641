{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFy6leD3ZmzS"
   },
   "source": [
    "# Fall 2019 CX4641/CS7641 Homework 1\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Sep 12, Thursday, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged, but each student must write his own answers and explicitly mention any collaborators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2Dz3TFIZmzT"
   },
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "In this assignment, we only have writing questions: you are asked to answer them in the markdown cells.\n",
    "\n",
    "- Graduate students are required to complete all the questions including **bouns parts**. Undergraduate students are welcome to try bouns questions and we will add them on your final grade.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "    \n",
    "- You could directly type the Latex equations in the markdown cell.\n",
    "\n",
    "- Typing with Latex is highly recommended. An image scan copy of handwritten also works. If you hand write, try to be clear as much as possible. No credit may be given to unreadable handwriting.\n",
    "    \n",
    "- If you want to add any picture to your answer, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vurwpE07ZmzU"
   },
   "source": [
    "## 1 Linear Algebra (25pts + 8pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6pSyvDOZmzU"
   },
   "source": [
    "### 1.1 Determinant and Inverse of Matrix [11pts]\n",
    "Given a matrix M:\n",
    "\n",
    "$$M = \\begin{bmatrix} \n",
    "  5 & 0 & 1 \\\\ \n",
    "  6 & 1 & 2 \\\\\n",
    "  0 & 4 & 3\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### - Calculate the determinant of M. [5pts]\n",
    "(Calculation process required)\n",
    "\n",
    "### Solution\n",
    "Determinant of M\n",
    "$$\n",
    "|M| = +5*(1*3-4*2) - 0*(6*3-2*0) + 1*(6*4-1*0)\n",
    "$$\n",
    "\n",
    "<br /> \n",
    "$$\n",
    "|M| =  5*(-5) - 0 + 24\n",
    "$$\n",
    "<br /> \n",
    "$$\n",
    "|M| = -25 + 24 \n",
    "$$\n",
    "<br /> \n",
    "$$\n",
    "|M| = -1\n",
    "$$\n",
    "\n",
    "### - Does the inverse of M exist? If so, calculate $M^{-1}$. [6pts]\n",
    "(Calculation process required)\n",
    "\n",
    "  (**Hint:** please double check your answer and make sure $M M^{-1} = I$)\n",
    "  \n",
    "  \n",
    "  \n",
    "### Solution\n",
    "\n",
    "Yes, the inverse exist as det(M) is non zero, which indicates it is not a singular matrix\n",
    "<br />\n",
    "Let's calculate the inverse of this matrix:\n",
    "\n",
    "$$inv(M) = { adj(M)^T/det(M) }$$\n",
    "\n",
    "Inverse is basically the adjucate(which is the transpose of the cofactor matrix) of the matrix divide by the determinant of the matrix\n",
    "\n",
    "$$M = \\begin{bmatrix} \n",
    "  5 & 0 & 1 \\\\ \n",
    "  6 & 1 & 2 \\\\\n",
    "  0 & 4 & 3\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$adj(M) = \\begin{bmatrix} \n",
    "  (-1)^0*(3-8) & (-1)^1*(6*3 - 0) & (-1)^2*(24-0) \\\\ \n",
    "  (-1)^1*(0-4) & (-1)^2*(15-0) & (-1)^3*(20-0) \\\\\n",
    "  (-1)^2*(0-1) & (-1)^3*(10-6) & (-1)^4*(5-0)\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "    \n",
    "    \n",
    "$$adj(M) = \\begin{bmatrix} \n",
    "  -5 & -18 & 24 \\\\ \n",
    "  4 & 15 & -20 \\\\\n",
    "  -1 & -4 & 5\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "    \n",
    "    \n",
    "$$adj(M)^T = \\begin{bmatrix} \n",
    "  -5 & 4 & -1 \\\\ \n",
    "  -18 & 15 & -4 \\\\\n",
    "  24 & -20 & 5\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "    \n",
    "    \n",
    "We have already calculated det(M) in the first step so,\n",
    "\n",
    "\n",
    "det(M) = -1\n",
    "\n",
    "$$inv(M) = \\frac{ adj(M)^T } {det(M) }$$\n",
    "\n",
    "$$inv(M) = ( 1/-1 )* \\begin{bmatrix} \n",
    "  -5 & 4 & -1 \\\\ \n",
    "  -18 & 15 & -4 \\\\\n",
    "  24 & -20 & 5\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$inv(M) = \\begin{bmatrix} \n",
    "  5 & -4 & 1 \\\\ \n",
    "  18 & -15 & 4 \\\\\n",
    "  -24 & 20 & -5\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "  \n",
    "### 1.2 Characteristic Equation [8pts] (BONUS)\n",
    "Consider the eigenvalue problem: \n",
    "  $$Ax =\\lambda x, x \\neq 0$$\n",
    "where $x$ is a non-zero eigenvector and $\\lambda$ is eigenvalue of $A$. Prove that the determinant $|A-\\lambda I|= 0$.\n",
    "\n",
    "(**Hint**: If a matrix is not full-rank (has linearly dependent columns), it is singular and non-invertible)\n",
    "\n",
    "\n",
    "### Solution\n",
    "\n",
    "Proof\n",
    "\n",
    "* we can write the eigen value problem in a new form by subtracting &\\lambda x from both side:\n",
    "\n",
    "$$Ax - \\lambda x =0$$\n",
    "\n",
    "* we can then multiply both side with identity matrix to get the charactorstic equation\n",
    "$$ AxI - \\lambda xI = 0$$\n",
    "\n",
    "* Since Matrix multiplication is distributive, so we can write\n",
    "\n",
    "$$ (A-\\lambda I)x = 0$$\n",
    "\n",
    "* Now this is a homogenous equation, and it already given to us that x is non zero eigen vector.\n",
    "\n",
    "\n",
    "* We can treat $(A-\\lambda I)$ as another matrix\n",
    "\n",
    "\n",
    "* Since this is a homogeneous equation and from the nullspace property we know that $x \\in N(A - \\lambda I)$, i.e the x has a non trivial nullspace, because zero is the trivial and that does not fulfill our basis.\n",
    "\n",
    "\n",
    "* Null space of $A-\\lambda I \\in N(x \\in R^n such that (A - \\lambda I)X = 0)$\n",
    "\n",
    "\n",
    "* Column of $A-\\lambda I$ will be linearly independent if there is we can write any row or columns in terms of linear expression of other rows or columns, which means the matrix should not be a full rank matrix, only than there will be a linear dependence.\n",
    "\n",
    "* And because the $A-\\lambda I$ is a invertibel matrix, which means essentially $det(A-\\lambda I)= 0$ \n",
    "\n",
    "\n",
    "**Another way to proof this**\n",
    "\n",
    "\n",
    "$$(A-\\lambda I)x =0$$\n",
    "\n",
    "This is a set of n homogenous equation. If $(A-\\lambda I)$ is an invertible matrix then we can multiply both side by\n",
    "$(A=\\lambda I)^-1$ to conclude that x =0, thereby suggesting the trivial solution. But by definition, x is a non zero eigen vector, which means in order to find the non trivial solution, $(A-\\lambda I)$ has to be non invertible.\n",
    "\n",
    "And a matrix is non invertible if \n",
    "\n",
    "$$det(A-\\lambda I) = 0$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 Eigenvalue [7pts]\n",
    "Following 1.2, given a matrix $A$:\n",
    "   $$A = \n",
    "   \\begin{bmatrix} \n",
    "    1 & r \\\\ \n",
    "    r & 1 \n",
    "    \\end{bmatrix}$$ \n",
    "    \n",
    "Calculate all the eigenvalues of $A$. (Calculation process required. Your answer should be expressed as a function of $r$.)\n",
    "\n",
    "\n",
    "### Solutions\n",
    "$$A = \n",
    "   \\begin{bmatrix} \n",
    "    1 & r \\\\ \n",
    "    r & 1 \n",
    "    \\end{bmatrix}$$ \n",
    "    \n",
    "We know from the charactorstic equation that\n",
    "\n",
    "$$ (A -\\lambda I)x = 0$$\n",
    "\n",
    "$$ det(A -\\lambda I) = 0$$\n",
    "\n",
    "$$A = \n",
    "   \\begin{bmatrix} \n",
    "    1-\\lambda & r \\\\ \n",
    "    r & 1 - \\lambda \n",
    "    \\end{bmatrix}$$ \n",
    "\n",
    "$$(1-\\lambda)(1-\\lambda)-r^2 = 0$$\n",
    "\n",
    "$$\\lambda^2 - 2\\lambda -(r^2-1) = 0$$\n",
    "\n",
    "Solving the above quadratic equation we will get:\n",
    "\n",
    "$$\\lambda = \\frac{ 2 +- \\sqrt{4+4(r^2-1)} }{2}$$\n",
    "\n",
    "$$\\lambda_1 = 1  + r $$\n",
    "\n",
    "$$\\lambda_2 = 1 - r $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.4 Eigenvector [7pts]\n",
    "Following 1.3, given that the $l_2$ norm of each eigenvector is 1, what are the eigenvectors of matrix $A$? For example, if an eigenvector is \n",
    "    ${v}=\\begin{bmatrix} \n",
    "    x1 \\\\ \n",
    "    x2 \n",
    "    \\end{bmatrix}$, then $||v||_2 = \\sqrt{x_1^2 + x_2^2} = 1$ (Calculation process required.)\n",
    "    \n",
    "    \n",
    "    \n",
    "### Solution\n",
    "\n",
    "\n",
    "Given condition for l2 norm :\n",
    "\n",
    "$$\\sqrt{x_1^2 + x_2^2} = 1$$\n",
    "\n",
    "Lets take $\\lambda_1 = 1+r $, then our characterstic equation will be:\n",
    "\n",
    "$$ \\begin{bmatrix} \n",
    "    -r & r \\\\ \n",
    "    r & -r \n",
    "    \\end{bmatrix}\\begin{bmatrix} \n",
    "    x_1 \\\\ \n",
    "    x_2  \n",
    "    \\end{bmatrix} = 0 $$\n",
    "    \n",
    "Solving this linear equation:\n",
    "\n",
    "$$-rx_1 + rx_2 = 0$$\n",
    "\n",
    "$$r(x_2-x_1)=0$$\n",
    "\n",
    "$$x_2 = x_1$$\n",
    "\n",
    "Putting this in our l2 norm equation we will get\n",
    "\n",
    "$$x_1^2 + x_2^2 =1$$\n",
    "\n",
    "$$2x_1^2 = 1$$\n",
    "\n",
    "$$x_1 = \\sqrt{\\frac{1}{2}}$$\n",
    "$$x_2 = \\sqrt{\\frac{1}{2}}$$\n",
    "\n",
    "So our **First Eigen vectore will be***:\n",
    "\n",
    "$$ X_1 = \\begin{bmatrix} \n",
    "    \\sqrt{\\frac{1}{2}} \\\\ \n",
    "    \\sqrt{\\frac{1}{2}} \n",
    "    \\end{bmatrix}$$\n",
    "    \n",
    "Lets consider our second eigen value: $\\lambda_2 = 1-r$\n",
    "\n",
    "Solving this using the characterstic equation:\n",
    "\n",
    "$$ \\begin{bmatrix} \n",
    "    r & r \\\\ \n",
    "    r & r \n",
    "    \\end{bmatrix}\\begin{bmatrix} \n",
    "    x_1 \\\\ \n",
    "    x_2  \n",
    "    \\end{bmatrix} = 0 $$\n",
    "\n",
    "\n",
    "$$rx_1 + rx_2 = 0$$   \n",
    "\n",
    "\n",
    "$$r(x_2+x_1)=0$$\n",
    "\n",
    "$$x_2 = -x_1$$\n",
    "\n",
    "Putting these values in the l_2 norm equation:\n",
    "$$x_1 = \\sqrt{\\frac{1}{2}}$$\n",
    "$$x_2 = -\\sqrt{\\frac{1}{2}}$$\n",
    "\n",
    "So our second eigen vector will be:\n",
    "\n",
    "\n",
    "$$ X_2 = \\begin{bmatrix} \n",
    "    \\sqrt{\\frac{1}{2}} \\\\ \n",
    "    -\\sqrt{\\frac{1}{2}} \n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7D5eQGGiZmzV"
   },
   "source": [
    "## 2 Expectation, Co-variance and Independence [25pts + 5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMqPtRogZmzV"
   },
   "source": [
    "Suppose $X, Y$ and $Z$ are three different random variables.\n",
    "Let $X$ obeys Bernouli Distribution. The probability disbribution function is\n",
    "    $$p(x)=\\left\\{\n",
    "    \\begin{array}{c l}\t\n",
    "         0.5 & x = c\\\\\n",
    "         0.5 & x = -c.\n",
    "    \\end{array}\\right.$$\n",
    "    $c$ is a constant here.\n",
    "Let $Y$ obeys the standard Normal (Gaussian) distribution, which can be written as $Y \\sim N(0,1)$. $X$ and $Y$ are independent. Meanwhile, let $Z = XY$.\n",
    "\n",
    "### - What is the Expectation and Variance of $X$? (in terms of $c$) [4pts]\n",
    "\n",
    "\n",
    "### Solution\n",
    "Expection:\n",
    "\n",
    "$$E(x) = \\sum\\limits_{i=1}^n x_i*p(x_i)$$\n",
    "$$E(x) = 0.5*c + 0.5*(-c)$$\n",
    "$$E(x) = 0$$\n",
    "\n",
    "Variance:\n",
    "\n",
    "$$V(x) = E(x^2) - E(x)^2$$\n",
    "$$E(x^2) =c^2*0.5 + c^2*0.5 - 0 $$\n",
    "$$E(x^2) = c^2$$\n",
    "\n",
    "\n",
    "### - Show that when $c=1$, $Z$ is a standard Normal (Gaussian) distribution, which means $Z \\sim N(0,1)$. [9pts]\n",
    "### Solution\n",
    "\n",
    "Let N(z) be a normal function defined as $P(Z>=z)$\n",
    "There is an inequality sign, because continous function we can define discrete probabilities.\n",
    "\n",
    "so,\n",
    "\n",
    "* Putting Z=XY as given\n",
    "$$P(Z>=x) = P(XY>=z)$$ \n",
    "\n",
    "* Calculating in terms of marginal distribution or expanding the Probability and the value of c is given as -1\n",
    "$$        = P(X=1)*P(XY>=z|X=1) + P(X=-1)*P(XY>=z|X=-1)$$\n",
    "* Putting values of respective probabilities\n",
    "$$        = 0.5*P(Y>=z|X=1) + 0.5*P(-Y>=z|X=-1)$$\n",
    "$$        = 0.5*P(Y>=z|X=1) + 0.5*P(Y<=-z|X=-1)$$\n",
    "\n",
    "* Now since Y obeys standard normal distribution then from the graph of its distribution we can P(Y>=z) == P(Y<=-z)\n",
    "\n",
    "$$        = 0.5*N(Y) + 0.5*N(Y)$$\n",
    "$$        = N(Y)$$\n",
    "\n",
    "Since N(Y) is a normal distribution and we started with our product of distributions, we can say that Z is also a normal distribution\n",
    "\n",
    "### - How should we choose $c$ such that Y and Z are uncorrelated (which means $Cov(Y,Z) = 0$)? [9pts]\n",
    "### Solution\n",
    "\n",
    "Let X and Y be any random variable then covariance(measure of dependence between 2 random variable) between these 2 random variable will be:\n",
    "\n",
    "$$Cov = E(XY) - E(X)*E(Y)$$\n",
    "\n",
    "So for our given question\n",
    "\n",
    "$$Cov = E(YZ) - E(Y)*E(Z)$$\n",
    "Putting value Z= XY in above\n",
    "\n",
    "$$Cov = E(XY^2) - E(Y)*E(XY)$$\n",
    "\n",
    "Since both X and Y are independent we can use $E(XY) = E(X)E(Y)$\n",
    "\n",
    "$$Cov  = E(X)*E(Y^2) - E(Y)^2*E(X)$$\n",
    "\n",
    "$$Cov = E(X)(E(Y^2)-E(Y)^2)$$\n",
    "\n",
    "We can easily figure out that $(E(Y^2)-E(Y)^2)$ is a variance of Y\n",
    "\n",
    "$$Cov = E(X)V(Y)$$\n",
    "\n",
    "We know that V(Y) of a normal distribution is 1\n",
    "\n",
    "\n",
    "$$Cov = E(X)$$\n",
    "\n",
    "Now,\n",
    "$$Cov = 0.5 *c + 0.5*(-c) = 0$$\n",
    "\n",
    "We can see that Expectation will be zero for any value of c, and covariance as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### - Are Y and Z independent? (Just clarify) [3pts]\n",
    "\n",
    "### Solution \n",
    "\n",
    "No, they are not independent as Y and Z are  not jointly Gaussian. Z is combination of a discrete Var and Normal var which makes it not a jointly Gaussian rather.\n",
    "Had it been 2 jointly gaussian distribution then they would have been independent.But here it isn't the case\n",
    "\n",
    "### - Show your conclusion for the above question with an example. **(Bouns)** [5pts]\n",
    "\n",
    "### Solution\n",
    "\n",
    "We have given X as a discrete distribution which has the probabilty of 0.5 at point c and -c, and also we have a Normal distribution Y with mean=0 and var =1. we then defined a new Distribution with Z=XY, where we proved above that Z will also be a normal distribution.Zero covariance denotes that they are uncorelated but it does not mean that they are independent.\n",
    "\n",
    "2 random variables X and Y are independent if,\n",
    "$$P(X|Y) = P(X)$$\n",
    "or \n",
    "$$P(Y|X) = P(Y)$$\n",
    "\n",
    "\n",
    "Now if Y and Z are independent than\n",
    "\n",
    "$$P(Y|Z) = P(Y)$$\n",
    "$$P(Y|XY) = P(Y)$$\n",
    "$$P(Y|cY) = P(Y) = P(Y|-cY)$$\n",
    "\n",
    "Now we clearly know from above that conditional probability of Y(continous distribution) at some discrete point will be 0 as it is continous distribution, however that is not equal to the marginal distribution of P(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHsbeDGNZmzX"
   },
   "source": [
    "## 3 Maximum Likelihood [25pts + 10pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSEe9QB9ZmzY"
   },
   "source": [
    "### 3.1 Discrete Example [15pts]\n",
    "Suppose you are playing two unfair coins. The probability of tossing a head is $2 \\theta$ for coin 1, and $\\theta$ for coin 2. You toss each coin for several times, and you get the following results:\n",
    "\n",
    "| Coin No. | Result    |\n",
    "|------|------|\n",
    "|   1  | head |\n",
    "|   2  | head |\n",
    "|   1  | tail |\n",
    "|   2  | tail |\n",
    "|   1  | head |\n",
    "|   2  | tail |\n",
    "\n",
    "### - What is the probability of tossing a tail for coin 1 ($p_{t1}$) and tossing a tail for coin 2 ($p_{t2}$) [3pts]?\n",
    "\n",
    "### Solution\n",
    "\n",
    "$$P(Tail| coin1) = 1/3$$\n",
    "$$P(Tail| coin2) = 2/3$$\n",
    "\n",
    "### - What is the likelihood of the data given $\\theta$ [6pts]?\n",
    "\n",
    "### Solution\n",
    "Likelihood is the joint probability for the experiment above. \n",
    "\n",
    "$$L(\\theta) = P(head from coin1)*P(head from coin2)*P(tail from coin1)*P(tail from coin2)*P(head from coin1)*P(tail from coin2)$$\n",
    "\n",
    "$$L(\\theta) = (2\\theta)*(\\theta)*(1-2\\theta)*(1-\\theta)*(2\\theta)*(1-\\theta)$$\n",
    "\n",
    "### - What is maximum likelihood estimation for $\\theta$ [6pts]?\n",
    "\n",
    "### Solution\n",
    "\n",
    "We can solve the likelihood function by calculting the partial derivative and finding maxima for the likelihood function\n",
    "\n",
    "$$L`(\\theta) = 4\\theta^2(1-\\theta)(12\\theta^2 -13\\theta +3)$$\n",
    "\n",
    "$$L`(\\theta) = 4\\theta^2(1-\\theta)(4\\theta-3)(3\\theta-1)$$\n",
    "\n",
    "$$L`(\\theta) =4\\theta^2(1-\\theta)(4\\theta-3)(3\\theta-1) = 0$$\n",
    "\n",
    "$$\\theta = 0,1,1/3,4/3$$\n",
    "\n",
    "\n",
    "Now the maximum value of the likelihood functions occurs at $\\theta = 1/3$\n",
    "\n",
    "Hence, $\\theta = \\frac{1}{3}$\n",
    "\n",
    "### 3.2 Continues Example [10pts] (BONUS)\n",
    "\n",
    "### Solution\n",
    "A uniform distribution in the range of $[a, b]$ is given by\n",
    "\n",
    "$$\n",
    "f(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{b-a}} & {a \\leq x \\leq b} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "### What is maximum likelihood estimation for $a$ and $b$?\n",
    "\n",
    "(You need to show the derivation of your answer.)\n",
    "\n",
    "( **Hint**: Think of two cases, where $x < max(x_1, x_2, ..., x_n)$ and $x \\ge max(x_1, x_2, ..., x_n).)$\n",
    "\n",
    "\n",
    "### Solution\n",
    "\n",
    "We have given:\n",
    "$$\n",
    "f(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{b-a}} & {a \\leq x \\leq b} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Since it is a uniform distribution which is a continous distribution, so its likelihood function can be calculated using the joint probability distribution\n",
    "\n",
    "$$\n",
    "f(x) = \\prod_\\limits{1}^n\\frac{1}{(b-a)^n}\n",
    "$$\n",
    "\n",
    "We will calculate its log likelihood by taking log both hand sides\n",
    "\n",
    "$$\n",
    "log(f(x)) = log((b-a)^-n)\n",
    "$$\n",
    "\n",
    "Our loglikelihood funtion:\n",
    "$$\n",
    "log(f(x)) = -nlog(b-a)\n",
    "$$\n",
    "\n",
    "\n",
    "Now we will maximize this log likeli hood function:\n",
    "taking the partial derivative of the funtion with respect to a first and then b\n",
    "\n",
    "with respect to a:\n",
    "$$\n",
    "L`(a) = \\frac{n}{(b-a)}\n",
    "$$\n",
    "\n",
    "\\similarly with respect to b:\n",
    "$$\n",
    "L`(a) = -\\frac{n}{(b-a)}\n",
    "$$\n",
    "\n",
    "Now since we have the derivatives wrt to a and b, we can estimate the parameters for maximum likelihood, for which the derivative tends to zero, so we need to \n",
    "\n",
    "The derivative wrt a is $\\frac{n}{(b-a)}$, which is a monotonically increasing function, which means that at the maximum value of a at this derivate will tends to zero.\n",
    "so minimum a would be greatest a possible, and mle would be:\n",
    "\n",
    "mle = min(x_1,x_2,...x_n)\n",
    "\n",
    "\n",
    "The derivative wrt b is $\\frac{n}{(b-a)}$, which is a monotonically decreasing function, which means that at the minimum value of b at this derivate will tends to zero.\n",
    "so maximum b would be minimum b possible, and mle would be:\n",
    "\n",
    "mle = max(x_1,x_2,...x_n)\n",
    "\n",
    "### 3.3 Maximum A Posteriori (MAP) [10pts]\n",
    "<img src = \"https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png\" style=\"width:400px;height:600px\"/>\n",
    "\n",
    "(Reference: xkcd.com)\n",
    "\n",
    "Suppose there exists an unknown parameter $\\theta$ that describe whether the sun will explode tomorrow. $\\theta = 1$ means the sun will explode and $\\theta = 0$ if it won't. The likelihood function is:\n",
    "$$\n",
    "P(yes|\\theta)=\\left\\{\\begin{array}{ll}{1/36} & {\\theta = 0} \\\\ {35/36} & {\\theta = 1}\\end{array}\\right.\n",
    "$$\n",
    "### - What is the maximum likelihood estimate of $\\theta$?[3pts]\n",
    "\n",
    "### Solution\n",
    "In order to maximize the likelihood function we can clearly notice from the likelihood function that the maximum probability we have is at \\theta = 1, so \\theta =1 would be the mle \n",
    "\n",
    "### - Maximum A Posteriori (MAP) estimator aims to maximize the value of $\\theta$ in $p(\\theta|yes)$. What is the MAP estimate of $\\theta$ given that $P(\\theta=0)\\gg P(\\theta=1)$? Comment on the result.[7pts]\n",
    "\n",
    "( **Hint**: You can use Bayes Rule to get $p(\\theta|yes)$ from the likelihood! )\n",
    "\n",
    "### Solution\n",
    "\n",
    "From the Bayes therem:\n",
    "\n",
    "$$\n",
    "P(\\theta|yes) = \\frac{P(Yes|\\theta)P(\\theta)}{P(Yes)}\n",
    "$$\n",
    "\n",
    "We know that for estimating the MAP or maximum a posteriori analysis:\n",
    "\n",
    "$$MAP \\propto P(Yes|\\theta)P(\\theta) $$ \n",
    "\n",
    "\n",
    "As the marginal probability will remain same, so we are not considering the denominator term.In order to maximize $MAP \\propto P(Yes|\\theta)P(\\theta)$, we know we have 2 values of $\\theta =0, \\theta=1$, now the likelihood will be maximum at $\\theta = 1$, but since we have given here that $P(\\theta=0)\\gg P(\\theta=1)$ and $P(\\theta)$ is in numerator of MAP equation so, MAP will be maximum for $\\theta = 0$\n",
    "\n",
    "To know more about MAP, refer to wiki page:\n",
    "https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ym8gpKMHZmzY"
   },
   "source": [
    "## 4 Information Theory [25pts + 7pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HSWti__DYx9"
   },
   "source": [
    "### 4.1 Marginal Distribution [4pts]\n",
    "Suppose the joint probability distribution of two binary random variables $X$ and $Y$ are given as follows.\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\\hline X | Y & {1} & {2} \\\\ \\hline 0 & {\\frac{1}{5}} & {\\frac{2}{5}} \\\\ \\hline 1 & {0} & \\frac{2}{5} \\\\ \\hline\\end{array}\n",
    "$$\n",
    "\n",
    "### - Show the marginal distribution of $X$ and $Y$, respectively. [4pts]\n",
    "\n",
    "### Solution\n",
    "\n",
    "Marginal distribution is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. \n",
    "\n",
    "So basically we sums up the columns and rows probabilities to get the marginal distribution of the $X$ and $Y$\n",
    "\n",
    "\n",
    "Marginal distribution accross X: $\\frac{3}{5} ( which is =  \\frac{2}{5}+\\frac{1}{5}),\\frac{2}{5}( which is = \\frac{2}{5}+0)$\n",
    "\n",
    "Marginal distribution accross Y: $\\frac{1}{5}( which is = \\frac{1}{5}+0),\\frac{4}{5}( which is = \\frac{2}{5}+\\frac{2}{5})$\n",
    "\n",
    "    \n",
    "### 4.2 Mutual Information and Entropy [21pts]\n",
    "Given a dataset as below.\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\\hline Day & Outlook & Temperature & Humidity & Wind & Play? \\\\ \\hline 1 & overcast & hot & normal & medium & yes \\\\ \\hline 2 & sunny & hot & high & weak & no \\\\ \\hline 3 & sunny & mild & normal & weak & yes \\\\ \\hline 4 & rain & cool & high & strong & no \\\\ \\hline 5 & overcast & cool & normal & strong & yes \\\\ \\hline 6 & rain & mild & normal & medium & no \\\\ \\hline 7 & sunny & mild & high & medium & yes\\\\ \\hline 8 & overcast & hot & normal & strong & no\\\\ \\hline 9 & rain & hot & high & weak & no\\\\ \\hline 10 & sunny & cool & normal & strong & yes\\\\\\hline\\end{array}\n",
    "$$\n",
    "\n",
    "We want to decide whether to play or not to play basketball on a certain day. Each input has four features ($x_1$, $x_2$, $x_3$, $x_4$): Outlook, Temperature, Humidity, Wind. The decision (play vs no-play) is represented as $Y$.\n",
    "\n",
    "### - Find entropy $H(Y)$. [4pts]\n",
    "### Solution\n",
    "\n",
    "  $$\n",
    "  H(Y) = \\sum\\limits_1^n P(y_i)log(\\frac{1}{P(y_i)})\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  P(Yes) = 0.5\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  P(NO) = 0.5\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  H(Y) = 0.5log(2) + 0.5log(2)\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  H(Y) = 1\n",
    "  $$\n",
    "  \n",
    "  We can also deduce the same from seein the dist, 50 per= YES and 50 per =NO, meaning maximum uncertainity, thereby maximum entropy\n",
    "### - Find conditional entropy $H(Y|x_1)$, $H(Y|x_4)$, respectively. [8pts]\n",
    "### Solution\n",
    "  \n",
    "  $$\n",
    "  H(Y|x_1) = \\sum\\limits_1^n P(Y,x_1)log(\\frac{1}{P(Y|x_1)})\n",
    "  $$\n",
    "  We know that\n",
    "  \n",
    "  P(Y= yes and Outlook = overcast) = $\\frac{2}{10}$\n",
    "  \n",
    "  P(Y= yes and Outlook = sunny)= $\\frac{3}{10}$\n",
    "  \n",
    "  P(Y= yes and Outlook = rain)= $0$\n",
    "  \n",
    "  P(Y= No and Outlook = overcast)= $\\frac{1}{10}$\n",
    "  \n",
    "  P(Y= No and Outlook = sunny)= $\\frac{1}{10}$\n",
    "  \n",
    "  P(Y= No and Outlook = rain)= $\\frac{3}{10}$\n",
    "  \n",
    "  P(Y=Yes|Outlook=overcast) = $\\frac{2}{3}$\n",
    " \n",
    "  P(Y=Yes|Outlook=sunny)= $\\frac{3}{4}$\n",
    "  \n",
    "  P(Y=Yes|Outlook=rain)= $0$\n",
    "  \n",
    "  P(Y=No|Outlook=overcast)= $\\frac{1}{3}$\n",
    "  \n",
    "  P(Y=No|Outlook=sunny)= $\\frac{1}{3}$\n",
    "  \n",
    "  P(Y=No|Outlook=rain)= $1$\n",
    "  \n",
    "  $$\n",
    "  H(Y|x_1) = \\frac{2}{10}log(\\frac{3}{2}) + \\frac{3}{10}log(\\frac{4}{3}) + \\frac{1}{10}log(3) + \\frac{1}{10}log(3)\n",
    "              + \\frac{3}{10}\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  H(Y|x_1) = 0.6\n",
    "  $$\n",
    "  \n",
    "  \n",
    "  $$\n",
    "  H(Y|x_4) = \\sum\\limits_1^n P(Y,x_4)log(\\frac{1}{P(Y|x_4)})\n",
    "  $$\n",
    "  We know that\n",
    "  \n",
    "  P(Y= yes and wind = medium) = $\\frac{2}{10}$\n",
    "  \n",
    "  P(Y= yes and wind = weak)= $\\frac{1}{10}$\n",
    "  \n",
    "  P(Y= yes and wind = strong)= $\\frac{2}{10}$\n",
    "  \n",
    "  P(Y= No and wind = medium)= $\\frac{1}{10}$\n",
    "  \n",
    "  P(Y= No and wind = weak)= $\\frac{2}{10}$\n",
    "  \n",
    "  P(Y= No and wind = strong)= $\\frac{2}{10}$\n",
    "  \n",
    "  P(Y=Yes|wind=medium) = $\\frac{2}{3}$\n",
    " \n",
    "  P(Y=Yes|wind=weak)= $\\frac{1}{3}$\n",
    "  \n",
    "  P(Y=Yes|wind=strong)= $\\frac{2}{4}$\n",
    "  \n",
    "  P(Y=No|wind=medium)= $\\frac{1}{3}$\n",
    "  \n",
    "  P(Y=No|wind=weak)= $\\frac{2}{3}$\n",
    "  \n",
    "  P(Y=No|wind=strong)= $\\frac{2}{4}$\n",
    "  \n",
    "\n",
    "$$\n",
    "H(Y|x_4) = \\frac{2}{10}log(\\frac{3}{2}) + \\frac{1}{10}log(\\frac{3}{1}) + \\frac{2}{10}log(2) + \\frac{1}{10}log(3)+\\frac{2}{10}log(\\frac{3}{2}) + \\frac{2}{10}log(\\frac{2}{1})    \n",
    "$$\n",
    "  \n",
    "  $$\n",
    "  H(Y|x_4) = 0.950\n",
    "  $$\n",
    "  \n",
    "  \n",
    "### - Find mutual information $I(x_1, Y)$ and $I(x_4, Y)$ and determine whether which one ($x_1$ or $x_4$) is more informative. [5pts]\n",
    "\n",
    "### Solution\n",
    "\n",
    "For I(x_1,Y)\n",
    "\n",
    "$$\n",
    "I(x_1,Y) = H(x_1) + H(Y) - H(x_1,Y)\n",
    "$$\n",
    "$$\n",
    "H(x_1) = 1.502\n",
    "$$\n",
    "$$\n",
    "H(Y) = 1\n",
    "$$\n",
    "$$\n",
    "H(x_1,Y) =2.166\n",
    "$$\n",
    "$$\n",
    "I(x_1,Y) = 1.502 + 1 - 2.166\n",
    "$$\n",
    "$$\n",
    "I(x_1,Y) = 0.34\n",
    "$$\n",
    "\n",
    "For I(x_4,Y)\n",
    "\n",
    "$$\n",
    "I(x_4,Y) = H(x_4) + H(Y) - H(x_4,Y)\n",
    "$$\n",
    "$$\n",
    "H(x_4) = 1.52\n",
    "$$\n",
    "$$\n",
    "H(Y) = 1\n",
    "$$\n",
    "$$\n",
    "H(x_4,Y) = 2.52\n",
    "$$\n",
    "$$\n",
    "I(x_4,Y) = 1.52 + 1 - 2.52\n",
    "$$\n",
    "$$\n",
    "I(x_4,Y) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "We can see from the above that I(x_1,Y) > I(x_14,Y), hence I(x_1,Y) is more informative\n",
    "\n",
    "\n",
    "\n",
    "### - Find joint entropy $H(Y, x_3)$. [4pts]\n",
    "\n",
    "### Solution\n",
    "\n",
    "P(Y= yes and humidity = normal) = $\\frac{4}{10}$\n",
    "  \n",
    "P(Y= yes and humidity = high)= $\\frac{1}{10}$\n",
    "  \n",
    "P(Y= No and humidity = normal)= $\\frac{2}{10}$\n",
    "  \n",
    "P(Y= No and humidity = high)= $\\frac{3}{10}$\n",
    "  \n",
    "$$\n",
    "H(Y,x_3) = \\sum P(Y,X_3)log(\\frac{1}{P(Y,x_3)})\n",
    "$$\n",
    "$$\n",
    "H(Y,x_3) = \\frac{4}{10}log(2.25) + \\frac{1}{10}log(10) +\\frac{2}{10}log(5) + \\frac{3}{10}log(3.33)\n",
    "$$\n",
    "$$\n",
    "H(Y,x_3) = 1.779\n",
    "$$\n",
    "  \n",
    "  \n",
    "### 4.3 Bonus Question [7pts]\n",
    "### - Suppose $X$ and $Y$ are independent. Show that $H(X|Y) = H(X)$. [2pts]\n",
    "\n",
    "### Solution\n",
    "\n",
    "We know that, mutual information is the information that is contained in both X and Y, and can be written as:\n",
    "\n",
    "\n",
    "$$\n",
    "I(Y,X) = H(X) - H(X|Y)\n",
    "$$\n",
    "\n",
    "Now since X and Y are independent, than there will be no mutual information or common information between X and Y.\n",
    "Hence\n",
    "\n",
    "$$\n",
    "I(Y,X) = 0\n",
    "$$\n",
    "\n",
    "Putting this value in first equation:\n",
    "\n",
    "$$\n",
    " 0 = H(X) - H(X|Y)\n",
    "$$\n",
    "\n",
    "hence\n",
    "\n",
    "$$\n",
    "H(X) = H(X|Y)\n",
    "$$\n",
    "\n",
    "\n",
    "### - Suppose $X$ and $Y$ are independent. Show that $H(X,Y) = H(X) + H(Y)$. [2pts]\n",
    "\n",
    "### Solution\n",
    "\n",
    "We know that by definition Joint entropy H(X,Y) is sum of entropy of X and Entropy of Y given X:\n",
    "\n",
    "$$\n",
    "H(X,Y) = H(X) + H(Y|X)\n",
    "$$\n",
    "  \n",
    "From our first proof we proved that:\n",
    "\n",
    "$$\n",
    "H(Y|X) = H(Y)\n",
    "$$\n",
    "\n",
    "Putting this in first equation\n",
    "\n",
    "\n",
    "$$\n",
    "H(X,Y) = H(X) + H(Y)\n",
    "$$\n",
    "\n",
    "### - Prove that the mutual information is symmetric, i.e., $I(X, Y) = I(Y, X)$ and $x_i \\in X, y_i \\in Y$ [3pts]\n",
    "\n",
    "### Solution\n",
    "\n",
    "We know that, mutual information is :\n",
    "\n",
    "$$\n",
    "I(X,Y) = \\sum P(X,Y)log(\\frac{P(X,Y)}{P(X)*P(Y)})\n",
    "$$\n",
    "$$\n",
    "I(Y,X) = \\sum P(Y,X)log(\\frac{P(Y,X)}{P(Y)*P(X)})\n",
    "$$\n",
    "\n",
    "\n",
    "Now we know that P(X,Y) = P(Y,X) which is the Probability of X and Y together, which is a symmatric function as the order does not matter when calculating probability of x and y.\n",
    "\n",
    "We can re write the Equation 1 as(BY PUTTING THE VALUE OF P(X,Y) = P(Y,X)):\n",
    "$$\n",
    "I(X,Y) = \\sum P(Y,X)log(\\frac{P(Y,X)}{P(Y)*P(X)})\n",
    "$$\n",
    "\n",
    "which eactly same as I(Y,X), hence\n",
    "\n",
    "I(X,Y) = I(Y,X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw1-template.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
